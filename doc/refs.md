<a name="week1"></a>
## Week 1: Knowledge Distillation

### Main References 
* [Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network."](https://arxiv.org/abs/1503.02531)

* [Mirzadeh, Seyed-Iman, et al. "Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher." ](https://arxiv.org/pdf/1902.03393)

* [Lopez-Paz, David, et al. "Unifying distillation and privileged information." ](http://leon.bottou.org/publications/pdf/iclr-2016.pdf)

### Further References

* [Sanh, Victor, et al. "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter." ](https://arxiv.org/abs/1910.01108)

* [Hubara, Itay, et al. "Quantized neural networks: Training neural networks with low precision weights and activations." ](http://www.jmlr.org/papers/volume18/16-456/16-456.pdf)

* [Polino, Antonio, Razvan Pascanu, and Dan Alistarh. "Model compression via distillation and quantization." ](https://openreview.net/pdf?id=S1XolQbRW)



<a name="week2"></a>
## Week 2: Lottery ticket hypothesis

### Main References 
* [Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." ](https://arxiv.org/abs/1803.03635)

* [Frankle, Jonathan, et al. "The Lottery Ticket Hypothesis at Scale." ](https://www.arxiv-vanity.com/papers/1903.01611/)

* [Zhou, Hattie, et al. "Deconstructing lottery tickets: Zeros, signs, and the supermask."  ](https://arxiv.org/abs/1905.01067)

### Further References

* [Gaier, Adam, and David Ha. "Weight Agnostic Neural Networks."   ](https://weightagnostic.github.io/)



<a name="week3"></a>
## Week 3: Computational learning theory (PAC Learning)

### Main References 
* ["Understanding Machine Learning: From Theory to Algorithms." Textbook by Shai Ben-David and Shai Shalev-Shwartz.](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)

* ["Foundations of Machine Learning (Adaptive Computation and Machine Learning series) Second edition" – 2018. Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar  ](https://mitpress.mit.edu/books/foundations-machine-learning-second-edition)

* ["An Introduction to Computational Learning Theory." Book by Michael Kearns and Umesh Virkumar Vazirani. ](https://www.amazon.com/Introduction-Computational-Learning-Theory-Press/dp/0262111934)


<a name="week4"></a>
## Week 4: Causality (Part 1)

### Main References 
* ["Causality: Models, Reasoning and Inference". 2009. Second edition. by Judea Pearl](http://bayes.cs.ucla.edu/BOOK-2K/)

* ["The Book of Why: The New Science of Cause and Effect." Book by Dana Mackenzie and Judea Pearl. 2018 ](https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X)

* ["Elements of Causal Inference: Foundations and Learning Algorithms." Book by Bernhard Schölkopf, Dominik Janzing, and Jonas Peters](https://mitpress.mit.edu/books/elements-causal-inference)


<a name="week5"></a>
## Week 5: Causality (Part 2)

### Main References 
* ["Causality: Models, Reasoning and Inference". 2009. Second edition. by Judea Pearl](http://bayes.cs.ucla.edu/BOOK-2K/)

* ["The Book of Why: The New Science of Cause and Effect." Book by Dana Mackenzie and Judea Pearl. 2018 ](https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X)

* ["Elements of Causal Inference: Foundations and Learning Algorithms." Book by Bernhard Schölkopf, Dominik Janzing, and Jonas Peters](https://mitpress.mit.edu/books/elements-causal-inference)


<a name="week6"></a>
## Week 6: Gaussian process

### Main References 
* ["Gaussian Processes for Machine Learning." Book by Carl Edward Rasmussen and Christopher K. I. Williams](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)

* ["Gaussian Processes: From the Basics to the State-of-the-Art." Dr. Richard E. Turner.](https://www.youtube.com/watch?v=92-98SYOdlY)

* [Görtler, et al., "A Visual Exploration of Gaussian Processes", Distill ](https://distill.pub/2019/visual-exploration-gaussian-processes)

* ["Deep Gaussian Processes." Damianou, A. and Lawrence, N., 2013. Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics](http://proceedings.mlr.press/v31/damianou13a.html)



<a name="week7"></a>
## Week 7: Successor Representation in Reinforcement Learning 

### Main References 
* [Dayan, Peter. "Improving generalization for temporal difference learning: The successor representation."  ](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1993.5.4.613)

* [Gershman, Samuel J. "The successor representation: its computational logic and neural substrates." ](https://www.jneurosci.org/content/38/33/7193)

* [Kulkarni, Tejas D., et al. "Deep successor reinforcement learning."](https://arxiv.org/abs/1606.02396)

* [Barreto, André, et al. "Successor features for transfer in reinforcement learning." ](https://papers.nips.cc/paper/6994-successor-features-for-transfer-in-reinforcement-learning.pdf)

* [The present in terms of the future: Successor representations in Reinforcement learning. Blog post by Arthur Juliani.](https://medium.com/@awjuliani/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3)

### Further References

* [Botvinick, Matthew, and Ari Weinstein. "Model-based hierarchical reinforcement learning and human action control."](https://royalsocietypublishing.org/doi/full/10.1098/rstb.2013.0480)
